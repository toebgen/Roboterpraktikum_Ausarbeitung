
\chapter{Einleitung}
\label{einleitung_cha}
\authorsection{\editorabel}

Das aktuelle Interesse in der Forschung zur Robotik und insbesondere der Servicerobotik ist Robotersysteme flexibler zu gestalten und für breitere Anwendungsgebiete, zum Beispiel statt eines reinen Staubsaugroboters einen Haushaltsroboters zu verwenden. Im Fokus steht außerdem, wie auch in der Informationstechnologie eine kostengünstige Verbesserung des Mensch-Maschine-Interface\cite{kinect_1}.

Die Forschungsfelder in Industrie- und Servicerobotik nähern sich dabei im Laufe der Jahre immer weiter an.
So müssen Industrieroboter in Zusammenarbeit mit dem Menschen ihre Aufgaben verrichten und Serviceroboter in Zukunft auch komplexe Handhabungsaufgaben lösen.
Die Entwicklung findet daher besonders im Bereich der Informationstechnologie und der Sensorik statt  \cite{kinect_2}.

In den letzten Jahrzehnten hat sich die Bildverarbeitung rasant weiterentwickelt.
Zur meist funktionsrelevanten Interaktion eines Serviceroboters mit der Umwelt muss ein Roboter aufgrund der Vielfalt möglicher Umgebungen adaptiv in der Lage sein reichhaltige Informationen darüber zu gewinnen.
Unstrukturierte Umgebungen lassen sich nicht vollständig modellieren.
Roboter müssen somit durch Interaktion mit ihrer Umwelt lernen, sich orientieren und auf Menschen bzw. Informationen reagieren.
Besonders anspruchsvoll sind dabei Perzeption und Verarbeitung von Signalen.



% \section{Motivation und Aufgabenstellung}
% \label{motivation_sec}
%\todo[inline]{Author hinzufügen}
%\authorsection{Kinect-Gruppe}
\authorsection{\editorjulian, \editortobias}

\section{Motivation}
\label{motivation_real_sec}


In vielen Fällen müssen sich Serviceroboter in unstrukturierten Umgebungen zurechtfinden, in denen es nicht möglich ist, diese durch
 (zum Beispiel markerbasierte) spezielle Kennzeichnung der Objekte mit Informationen anzureichern \citep{sturm10rss-workshop}. Aber nicht nur Roboter müssen sich in solchen Umgebungen zurechtfinden, auch Menschen müssen eine neue Umgebung zuerst kennenlernen. In vielen Situation, etwa bei der Besichtigung eines Museums oder einer Firma, wird auf das Konzept des Zeigens oder Vormachen zurückgegriffen. Eine Person die mit der konkreten Situation oder Umgebung vertraut ist, teilt ihr Wissen mit anderen Personen in dem sie eine Führung anbietet oder den Weg zu einem bestimmten Ziel zeigt.

In modernen Forschungseinrichtungen, wie etwa dem \gls{fzi} in Karlsruhe, wird besonderen Wert auf das Sehen und Anfassen aktueller Forschungsprojekte gelegt\cite{fziHoLL}.
Um Besuchern die Räumlichkeiten und Forschungsprojekte zu zeigen ist es hier ebenfalls notwendig Führungen anzubieten. Um die Erfahrung Servicerobotik zu intensivieren liegt es auf der Hand eine solche Führung von einem Roboter ausführen zu lassen. Und um noch einen Schritt weiter zu gehen bietet es sich an die Definition einer Route, welche der Roboter späteren Besuchern zeigen soll, nicht fest zu programmieren, sondern die Möglichkeit zu schaffen, dass es dem Roboter möglich ist einem Menschen zu folgen und von ihm diese Route einzulernen. Somit schließt sich der Kreislauf; der Roboter lernt vom Menschen und Besucher können wiederum vom Roboter lernen.
 
% Auch Industrieroboter sollen in Zukunft enger mit (unberechenbaren) menschlichen Kooperationspartnern zusammenarbeiten.
% Kamerabilder stellen in diesem Kontext eine reichhaltige und kostengünstige Informationsquelle dar und können über Sensorfusion bzw. Musterprojektion zusätzlich
% zu 3D-Tiefenbildern erweitert werden. 
%
%Das Identifizieren und Verfolgen geometrischer Objekte in (Tiefen-) Bildern ist ein weit entwickeltes Feld mit hoher Reife und etablierten Methoden.
% Die große Herausforderung ist die semantische Interpretation dieser Daten. Besonders das zuverlässige Gewinnen funktionsrelevanter Informationen,
% wie zum Beispiel interaktiver Charakteristika, kinematischer, werkstofftechnischer und  Oberflächen-Beschaffenheit aus (Tiefen-) Bildern stellt sich
% als problematisch dar. In unstrukturierten Umgebungen jedoch versagen andere Sensorentypen oder deren Anwendung ist bisher aufgrund des Preises sinnlos.
% 
%Aktuell werden verschiedene Ansätze verfolgt: Zum einen generiert und erfasst man weitere Sensorinformationen durch Interaktion eines Roboters mit der Umgebung,
% zum anderen werden dem Roboter weitreichende erwartete Zusammenhänge von Informationen bereitgestellt. Entwickler können bei diesen Aufgaben auf weit entwickelte Roboterplattformen
% zurückgreifen, wie den PR2 der Firma willow garage und wie in diesem Praktikum
% \gls{hollie} des \gls{fzi} Karlsruhe. Willow garage stellt auch umfangreiche
% Software zur (Tiefen-) Bildverarbeitung und Robotersteuerung zur Verfügung,
% wie \gls{opencv}, \gls{pcl} und \gls{ros}.

\section{Aufgabenstellung}
\label{einleitung_aufgabenstellung_sec}
\authorsection{\editorjulian, \editortobias}

Das Szenario, welches im Rahmen des Praktikums bearbeitet werden soll, ist folgendes:
Besucher des \gls{fzi} sollen in Zukunft von der mobilen Roboterplattform \gls{hollie} (s. Abb. \ref{fig:hollie}) durch das Gebäude geführt werden können.
Dazu ist es notwendig, dass der Roboter einen Pfad einlernen, und diesen anschließend einem Besucher vorführen kann.
Die durch Dynamik geprägte Büroumgebung setzt zudem voraus, dass eine Reaktion des Roboters auf neue Hindernisse erfolgt.
Dem Roboter muss es selbstständig möglich sein, Hindernisse, die auf dem Pfad liegen, zu umfahren, und einen kolissionsfreien Weg zu finden.
Zur intuitiven Kontrolle des Roboters soll die Steuerung mittels Gesten möglich sein.

Zusammenfassend muss der Roboter folgende Aufgaben bewältigen können:

\begin{itemize}
  \item Bedienung von \gls{hollie} mittels Gesten
	\begin{itemize}
	\item Gestenerkennung durch Kinect
	\end{itemize}
  \item Lokalisierung des Roboters in gegebener Karte
  \item Verfolgung des Menschen
	\begin{itemize}
	  \item Tracking des Menschen (Position und Geschwindigkeit)
	  \item Bahnplanung von Roboter zu Mensch
	  \item Speicherung des Pfads
	\end{itemize}
  \item Abfahren des gespeicherten Pfads
	\begin{itemize}
	  \item Rückkehr zum Startpunkt
	  \item Erneutes Abfahren (vorwärts)
	\end{itemize}
  \item Hindernis-Umfahrung
	\begin{itemize}
	  \item Bahnplanung
	  \item Protective-Field
	\end{itemize}
\end{itemize}

\begin{figure}[h]
	\center
	\includegraphics[width=0.5\textwidth]{graphics/hollie}
	\caption{\label{fig:hollie} Die mobile Roboterplattform \gls{hollie}}
\end{figure}



\section{Stand der Technik}
\label{stand_der_technik_sec}
\authorsection{\editorabel}

\subsection{Hardware}

\subsubsection{3D Kameras (am Beispiel Microsoft Kinect)}
Die Abbildung \ref{fig:Kinect} zeigt ein Bild der Kinect.
Sie ist mit laser-basiertem \gls{ir} Projektor und einem monochromen \gls{cmos} Sensor ausgestattet.
Der \gls{ir}-Projektor projiziert ein Muster auf die beobachtete Szenerie, während der  Sensor die Verformungen des Musters detektiert und gegen ein Vorgabemuster mittels Epipolargeometrie ein Tiefenbild errechnet.
Die Kinect ist darüber hinaus mit einem Neigemotor, einer Reihe von Mikrofonen und einer \gls{rgb}-Kamera ausgestattet.
Horizontal hat die Kamera einen Öffnungswinkel von $57^\circ$ und vertikal $43^\circ$.
Die Arbeitsentfernung der Kinect liegt zwischen \unit[0,8]{m} und \unit[3,5]{m}.
Die Auflösung der Tiefenkamera beträgt \unit[3]{mm} in der X/Y Ebene und \unit[10]{mm} in Z-Richtung bei einem Abstand von \unit[2]{m}.
Die Auflösung ist $640 \times 480$ Bildpunkte bei einer Bildfrequenz von \unit[30]{Hz}.
Das Tiefenbild hat eine \unit[11]{bit} Auflösung mit Werten zwischen 0 und 2047.

% Die Abbildung \ref{fig:Kinect} zeigt ein Bild der Kinect. Sie ist mit
%  laser-basiertem \gls{ir} Projektor und einem monochromen \gls{cmos} Sensor ausgestattet.
%  Der \gls{ir}-Projektor projiziert ein Muster auf die beobachtete Szenerie,
%  während der \gls{cmos} Sensor die Verformungen des Musters detektiert und gegen
%  ein Vorgabemuster mittels Epipolargeometrie ein Tiefenbild errechnet.
%  Die Kinect ist darüber hinaus mit einem Neigemotor, einer Reihe von Mikrofonen
%  und einer \gls{rgb}-Kamera ausgestattet. Horizontal hat die Kamera einen Öffnungswinkel
%  von $57^\circ$ und vertikal $43^\circ$. Die Arbeitsentfernung der Kinect liegt
%  zwischen $0,8$ und $3,5m$. Die Auflösung der Tiefenkamera beträgt $3mm$ in
%  der X/Y Ebene und $10mm$ in Z-Richtung bei einem Abstand von $2$ Metern. Die
%  Auflösung ist $640 \times 480$ Bildpunkte bei einer Bildfrequenz von $30Hz$. Das
%  Tiefenbild hat eine $11bit$ Auflösung mit Werten zwischen $0$ und $2047$.

\begin{figure}[h]
\center
\includegraphics[scale=0.3]{graphics/Kinect.jpg}
\caption{\label{fig:Kinect} Kinect \cite{kinect_1}}
\end{figure}

Für die Kinect wurden in den letzten Jahren mehrere Entwicklerbibliotheken erstellt, die sich wachsender Beliebtheit erfreuen.
Es ist eine rasant wachsende Anzahl von Applikationen vorhanden, die 3D Kameras unterstützen.
Ursprünglich als Entertainmenthardware entwickelt, findet die Kinect vermehrt auch in der Forschung massiv Anwendung.
Ähnliche Spezifikationen haben auch die Wavi Xtion Kameras der Firma ASUS.



\subsubsection{Mobile Roboterplattformen}

Mobile Roboterplattformen werden in den meisten Fällen zu Forschungszwecken verwendet.
Wissenschaftler verschiedener Disziplinen wirken an Design und Programmierung mit.
Sie weisen teils stark unterschiedliche Konfigurationen auf.
Seit vielen Jahren wird jedoch intensiv an menschenähnliche Prototypen gearbeitet.
Nach der Konstruktion dienen solche Plattformen häufig dazu durch verschiedenste Programmierungen vorgegebene Aufgaben zu erfüllen.
Im Rahmen des Mobile Roboterpraktikums haben wir an der Plattform HoLLiE des FZI Karlsruhe gearbeitet.
Auch die Roboterplattform ARMAR wurde in Karlsruhe am KIT entwickelt.

Aufgrund der großen Popularität, technischen Ausgereiftheit und guten Dokumentation soll exemplarisch an dieser Stelle die Roboterplattform PR2 der Firma willow garage dargestellt werden, siehe Abbildung  \ref{fig:PR2Struktur}.

\begin{figure}[h]
\center
\includegraphics[scale=0.5]{graphics/PR2Struktur.jpg}
\caption{\label{fig:PR2Struktur} Struktur des PR2 \cite{kinect_5}}
\end{figure}

Der PR2 ist ein kommerziell erhältlicher, weit entwickelter humanoider Roboter, der den typischen Aufbau humanoider Roboterplattformen aufweist.
Er hat folgende, HoLLiE ähnlicher, technische Eigenschaften \cite{kinect_5}:

\begin{itemize}
  \item 2 Arme mit Greifern (Arm 4 \gls{dof}, Handgelenk 3 \gls{dof}, Greifer 1 \gls{dof})
  \item Kopf mir 2 \gls{dof}
  \item Omnidirektionale Basis (3 \gls{dof})
\end{itemize}

Außerdem ist der PR2 mit einer großen Anzahl an Sensoren ausgestattet und innerhalb gewisser Parameter modular konfigurierbar.
Der PR2 ist mit folgenden Sensoren ausgestattet bzw. ausstattbar:

\begin{itemize}
  \item Microsoft Kinect
  \item 5 MP Kamera
  \item Stereokamerasystem
  \item Kleinwinkelkamera
  \item \gls{led} Musterrojektor
  \item Laserscanner
  \item Mikrofonarray
  \item Beschleunigungssensorik in den Armen
  \item Kraftsensorik in den Greifern
  \item Kalibrierungs-\glspl{led}
\end{itemize}

Der PR2 wird mittlerweile an einigen namhaften Universitäten und Forschungsstätten eingesetzt (\ua TU München, Stanford University und \gls{mit}) und es existieren Arbeiten in verschiedensten Bereichen, wie zur Betreuung älterer und eingeschränkter Menschen, zum Verrichten von Hausarbeiten  und dem Einsatz in Dienstleistungsbetrieben.

Besonders an dem Roboter ist die gemeinsame mechatronische, aber besonders informationstechnische Plattform, die auf dem Betriebssystem \gls{ros} aufbaut und es Wissenschaftlern ermöglicht in einer gemeinsamen Umgebung zu arbeiten, Ergebnisse auszutauschen und auf neue Generationen zu übertragen.



\subsection{Software}

\subsubsection{OpenNI}

\gls{openni} dient zur Entwicklung von Applikationen der Mensch-Maschine-Interaktion.
Dazu stehen der \gls{api} viele Schnittstellen zur Verfügung.
Folgende Eigenschaften stellt die \gls{api} zur Verfügung \cite{kinect_6}:

\begin{itemize}
  \item Kommunikation mit Video- und Audiosensoren
  \item Middleware zur Video- und Audiodatenverarbeitung
\end{itemize}

Dem Entwickler stehen zur Entwicklung entsprechender Appliaktionen mit \gls{openni} eine Reihe von Werkzeugen zur Verfügung.Grundlegend ist dabei, dass ein gemeinsames Interface in Form von Middleware verwendet wird.
Dies ermöglicht die Entkopplung von Hardware- und Softwareherstellern, die nur die spezifizierten Datensignale liefern, bzw. darauf aufbauen müssen.
\gls{openni} steht zum kostenlosen Download zur Verfügung.
Die Struktur ist in unten stehender Abbildung dargestellt: 

\begin{description}
 \item[Oben:] Software-Applikationen die mit \gls{openni} kommunizieren
 \item[Mitte:] \gls{openni}, das Kommunikationsschnittstellen sowohl zu Hardware- als auch zu Softwarekomponenten zur Verfügung stellt
 \item[Unten:] Hardwarekomponenten, die Video- und Audiosignale aufnehmen
\end{description}

\begin{figure}[h]
	\center
	\includegraphics[width=0.7\textwidth]{graphics/openNI.jpg}
	\caption{\label{fig:openNI} \gls{openni} Architektur \cite{kinect_6}}
\end{figure}

\gls{openni} basiert auf drei Grundkonzepten:
\begin{itemize}
  \item Modularität
  \item Produktionsknoten
  \item Produktionsketten
\end{itemize}


\paragraph{Modularität}
Modularität bezieht sich in diesem Zusammenhang auf alle Aspekte OpenNIs.
Zum einen Hardware-Modularität;
Theoretisch werden alle Sensoren, die die spezifizierten Signale liefern unterstützt.
Auch die Software ist modular einsetzbar mit Komponenten zur Hand-, Körper-, Gesten- und Szenendetektion.
Mit den aufgezählten Modulen lassen sich relevante Informationen wie Gelenkposen, erkannte Gesten und die Segementierung der Szene gewinnen \cite{kinect_6}.

\paragraph{Produktionsknoten}
Produktionsknoten dienen in \gls{openni} zur Generierung von Daten.
Diese Daten stellen sie anderen Anwendungen zur Verfügung.
Da Prduktionsknoten in einer Hierarchie zu einander stehen können Produktionsknoten einer niedrigeren Hierarchieebene von solchen einer höheren Ebene genutzt werden wodurch.
Somit lassen sich von manchen Knoten low-level-Informationen wie Tiefenbilder, von anderen high-level-Informationen wie Gelenkposen generieren \cite{kinect_6}.

\paragraph{Produktionsketten}
Produktionskette ist der formalisierte Begriff für die Verarbeitungskaskade in einer Produktionsknotenhierarchie.
Höhere stehende Produktionsknoten fragen Informationen bei niedriger gestellten ab, die wieder niedriger gestellte abfragen usw.
Anwendern bleibt somit, mit der Verwendung der abstrakten Produktionsknoten hoher Hierarchieebenen die komplexe Verarbeitunsstruktur verborgen.
Diese Struktur ermöglicht Entwicklern außerdem eine größere Freiheit bei deren Gestaltung \cite{kinect_6}.

\paragraph{Microsoft SDK}
Auch der Hersteller der Kinect, Microsoft, bietet nun ein offizielles \gls{sdk}, das sich leicht in Visual Studio und Dot Net integrieren lässt.
Mit dem Kinect Sensor und Microsoft \gls{sdk} hat man Zugang zu Werkzeugen mit denen Menschen und Sprache erkannt werden.
Darüber hinaus existiert ein Entwicklerportal mit kostenlosen Tutorials und Beispielen.
Dort kann Microsoft \gls{sdk} auch kostenlos heruntergeladen werden \cite{kinect_4}.
Die Eigenschaften des Microsoft \gls{sdk} sind denen von \gls{openni} ähnlich, Microsoft \gls{sdk} bietet jedoch kein eigenes Gestenerkennungsmodul, wie es in \gls{openni} verfügbar ist.
In Vergleichstests von \gls{openni} und Microsoft \gls{sdk} schneidet \gls{openni} bisher besser ab, jedoch liefert Microsoft \gls{sdk} höhere Genauigkeit und Robustheit beim Skeletttracking.
Die Sprach- und Gestenerkennung findet nicht nur in Spielen sondern auch in der Bedienung des Computers Anwendung, wie zum Beispiel bei Microsofts Internetsuchmaschine bing.
Eine neu entwickelte Körperscan Software erlaubt es, Bilder aufzunehmen und diese als Animationen oder als Avatar in Spielen zu verwenden, siehe Abbildung \ref{fig:skelettracking} \cite{kinect_5}.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.8]{graphics/skelettracking.jpg}
	\caption{\label{fig:skelettracking} Beispielhafte Anwendung von Skelettracking\cite{kinect_5}}
\end{figure}


%\gls{openni} ist ein plattformübergreifendes Framework zur Entwicklung von
%Applikationen, die auf natürlicher Mensch-Maschine-Interaktion aufbauen.
% Die \gls{openni}-\gls{api} stellt eine Vielzahl an Schnittstellen bereit, um
% entsprechende Applikationen zu programmieren. Der Hauptzweck von \gls{openni}
%  ist eine Standard-\gls{api} bereitzustellen, die folgende Eigenschaften vereint \citep{openNI2012}:
%
%\begin{itemize}
%  \item Kommunikation mit Video und Audiosensoren
%  \item Bereitstellung von Middleware zur Video- und Audiodatenverarbeitung,\\
%  wie die Unterstützung bei der Objektdetektion in Bildern
%\end{itemize}
%
%Dabei stellt \gls{openni} eine Reihe von \glspl{api} bereit, die sowohl von  (3D-Kamera-)
% Sensoren als auch Middleware Komponenten implementiert werden können.
% Durch das Entkoppeln von Sensor und Middleware können mit \gls{openni} angefertigte Softwareprodukte
% ohne zusätzliche Arbeit auf verschiedener Middleware aufbauen (\glqq write once, deploy everywhere\grqq).
% Middleware Entwickler können somit Algorithmen, die auf standardisierten (Bild-) Formaten aufbauen,
% unabhängig von der Sensorik-Hardware entwickeln. Hardware Hersteller müssen auf der anderen Seite
% Sensoren zur Verfügung stellen, die diese Bildformate generieren können. \gls{openni} steht kostenlos
% als open source Quelle zur Verfügung. In Abbildung \ref{fig:openNI} ist das
% Konzept von \gls{openni} in 3 Schichten dargestellt:
%
%\begin{description}
%  \item[Oben:] Software, die Natürliche Interaktion auf \gls{openni} aufbauend
%  verwendet
%  \item[Mitte:] \gls{openni}, das Kommunikationsschnittstellen sowohl zu
%  Hardware- als auch zu\\ Softwarekomponenten zur Verfügung stellt
%  \item[Unten:] Hardwarekomponenten, die Video- und Audiosignale aufnehmen
%\end{description}
%
%\begin{figure}[h]
%\center
%\includegraphics[scale=0.8]{graphics/openNI.jpg}
%\caption{\label{fig:openNI} \gls{openni} Architektur \citep{openNI2012}}
%\end{figure}
%
%OpenNI basiert auf einigen Grundkonzepten:
%
%\begin{itemize}
%  \item Modularität
%  \item Produktionsknoten
%  \item Produktionsketten
%\end{itemize}
%
%\newpage
%\todo[inline]{Ein wenig gepfuscht hier \ldots falls wer Lust hat kann er das
%ja ändern}
%\begin{description}
%\item[Modularität]
%\end{description}
%
%Unter Modularität ist in diesem Zusammenhang vor allem die Austauschbarkeit von Komponenten
% und die Architektur des Systems zu verstehen.
% Es werden folgende Module unterstützt:
%
%
%\begin{itemize}
%\item \textbf{Sensoren:}
%\begin{itemize}
%  \item 3D Sensoren
%  \item \gls{rgb} Kameras
%  \item Infrarot Kameras
%  \item Mikrofone / Mikrofonarrays
%\end{itemize}
%\item \textbf{Middleware Komponenten:}
%\begin{description}
%  \item[Ganzkörperanalyse Middleware:] Softwarekomponente, die Sensordaten
%  verarbeitet und daraus relevante Informationen wie Gelenkposen ableitet
%  \item[Handanalyse Middleware:] Software, die hilft Sensordaten zu verarbeiten,
%  um die Positionen von Händen zu ermitteln
%  \item[Gestendetektion:] Softwarekomponente, die Gesten erkennt und diese
%  Information an die Anwendungen weitergibt
%  \item[Szenenanalyse:] Software, die verschiedene Szenenrelevanten
%  Informationen ableitet, wie Hintergrundseparation, Erkennen der Bodenebene und Identifikation und Detektion von verschiedenen Personen
%\end{description}
%\end{itemize}
%
%
%\begin{description}
%\item[Produktionsknoten]
%\end{description}
%
%
%In \gls{openni} werden sogenannte Produktionsknoten definiert die die Aufgabe haben,
% für verschiedene Anwendungen die Daten zur Verfügung zu stellen.
% Produktionsknoten stehen dabei in einer Hierarchie zueinander.
% Sie können sowohl Produktionsknoten eines niedrigeren Levels verwenden als auch von Knoten höheren
% Levels verwendet werden. Konkret bietet \gls{openni} einen User Generator, der alle Daten einer getrackten Person
% liefern kann. Hierfür verwendet der User Generator einen Tiefenbildgenerator, aus dem er seine Informationen ableitet.
%
%\begin{description}
%\item[Produktionsketten]
%\end{description}
%
%Wie bereits dargestellt können mehrere Module gleichzeitig an der selben \gls{openni} Distribution angemeldet sein.
% Diese Topologie erlaubt Anwendungen flexibel auf Aufnahme- und Verarbeitungsmodule zuzugreifen.
% Als Produktionsketten werden die bereits angesprochenen hierarchischen Verarbeitungsketten der Produktionsknoten verstanden.
% Der erwähnte User Generator verwendet einen Tiefengenerator, der wiederum seine Daten von einem Tiefensensor bezieht.
% Diese Abhängigkeit bzw. sequenzielle Herunterbrechen der Aufgabe der Produktionsknoten wird als Produktionskette verstanden.
% Vorteilhaft ist an dieser Struktur, dass Anwendungen nur die oberste Schicht mit ihren Fähigkeiten nutzt.
% Die Konfiguration der darunter liegenden Schichten bleibt dabei verborgen und erlaubt Herstellern größere Freiheit
% in der Gestaltung der niedrigeren Hierarchieebenen \citep{openNI2012}.
%
%\subsubsection{Microsoft \glsentrytext{sdk}}
%
%Auch der Hersteller der Kinect, Microsoft, bietet nun ein offizielles \gls{sdk},
% das sich leicht in Visual Studio und Dot Net integrieren lässt. Mit dem Kinect Sensor und Microsoft \gls{sdk}
% hat man Zugang zu Werkzeugen, mit denen Menschen und Sprache erkannt werden.
% Darüber hinaus existiert ein Entwicklerportal mit kostenlosen Tutorials und Beispielen.
% Dort kann Microsoft \gls{sdk} auch kostenlos heruntergeladen werden \citep{kinectDevKfW2012}.
%
%Die Sprach- und Gestenerkennung findet nicht nur in Spielen sondern auch in der Bedienung des Computers Anwendung,
% wie zum Beispiel bei Microsofts Internetsuchmaschine bing\footnote{\url{www.bing.com/}}. Eine neu entwickelte Körperscan Software erlaubt es,
% Bilder aufzunehmen und diese als Animationen oder als Avatar in Spielen zu
% verwenden, siehe Abbildung \ref{fig:skelettracking} \citep{noGameKinect2012}.
%
%\begin{figure}[h]
%\center
%\includegraphics[scale=0.8]{graphics/skelettracking.jpg}
%\caption{\label{fig:skelettracking} Beispielhafte Anwendung von Skelettracking
%\citep{noGameKinect2012}}
%\end{figure}




